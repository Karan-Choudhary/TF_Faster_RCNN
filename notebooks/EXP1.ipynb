{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Generation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_cor(anchor):\n",
    "    \"\"\"\n",
    "    Return width, height, x center, and y center for an anchor (window).\n",
    "    \"\"\"\n",
    "    # anchor is 1X4\n",
    "    w = anchor[2] - anchor[0] + 1\n",
    "    h = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + 0.5 * (w - 1)\n",
    "    y_ctr = anchor[1] + 0.5*(w-1)\n",
    "    return w,h,x_ctr,y_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_anchors(ws,hs,x_ctr,y_ctr):\n",
    "    \"\"\"\n",
    "    Given a vector of widths (ws) and heights (hs) around a center (x_ctr,y_ctr),\n",
    "    Return a set of anchor boxes in (w, h, x_ctr, y_ctr) format.\n",
    "    \"\"\"\n",
    "    # ws,hs,x_ctr,y_ctr are numpy arrays\n",
    "    w = ws[:,np.newaxis] # [1,4] -> [4,1] i.e it generate array of one more dimension\n",
    "    # print('w in mkanchors: ',w)\n",
    "    h = hs[:,np.newaxis] # [1,4] -> [4,1]\n",
    "    # print('h in mkanchors: ',h)\n",
    "    anchors = np.hstack((x_ctr - 0.5*(w-1),\n",
    "                                            y_ctr - 0.5*(h-1),\n",
    "                                            x_ctr + 0.5*(w-1),\n",
    "                                            y_ctr + 0.5*(h-1))) # horizontal stack\n",
    "    # print('anchors in mkanchors: ',anchors)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ratio_enum(anchor,ratios):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n",
    "    \"\"\"\n",
    "    \n",
    "    w,h,x_ctr,y_ctr = _extract_cor(anchor)\n",
    "    # print(w,h,x_ctr,y_ctr)\n",
    "    size = w*h\n",
    "    # print('size='+str(size))\n",
    "    size_ratios = size/ratios\n",
    "    # print('size_ratios='+str(size_ratios))\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    # print('ws='+str(ws))\n",
    "    hs = np.round(ws*ratios)\n",
    "    # print('hs='+str(hs))\n",
    "    anchors = _make_anchors(ws,hs,x_ctr,y_ctr)\n",
    "    # print('anchors in ratio_enu='+str(anchors))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_enum(anchor,scales):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each scale wrt an anchor.\n",
    "    \"\"\"\n",
    "    w, h, x_ctr, y_ctr = _extract_cor(anchor)\n",
    "    # print(\"anchor in scle_enum=\"+str(anchor))\n",
    "    ws = w*scales\n",
    "    hs = h*scales\n",
    "    anchors = _make_anchors(ws,hs,x_ctr,y_ctr)\n",
    "    # print(\"Final anchors in scale_enum=\"+str(anchors))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  It generates 9 anchor boxes from a base anchor box\n",
    "\n",
    "def generate_anchors(base_size=16,ratios=[0.5,1,2],scales=np.array([8,16,32])):\n",
    "    base_anchor = np.array([1,1,base_size,base_size]) - 1\n",
    "    # print(base_anchor)\n",
    "    ratio_anchors = _ratio_enum(base_anchor,ratios)\n",
    "    # print(ratio_anchors.shape)\n",
    "    anchors_list=[]\n",
    "    for i in range(ratio_anchors.shape[0]):\n",
    "        anc = _scale_enum(ratio_anchors[i,:],scales)\n",
    "        anchors_list.append(anc)\n",
    "    anchors = np.vstack(anchors_list)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -84. ,  -34.5,   99. ,   60.5],\n",
       "       [-176. ,  -82.5,  191. ,  108.5],\n",
       "       [-360. , -178.5,  375. ,  204.5],\n",
       "       [ -56. ,  -56. ,   71. ,   71. ],\n",
       "       [-120. , -120. ,  135. ,  135. ],\n",
       "       [-248. , -248. ,  263. ,  263. ],\n",
       "       [ -36. ,  -85.5,   51. ,   89.5],\n",
       "       [ -80. , -173.5,   95. ,  177.5],\n",
       "       [-168. , -349.5,  183. ,  353.5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create uniformly spaced grid with spacing equal to stride\n",
    "\n",
    "def generate_anchors_pre_tf(height, width, feat_stride=16, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n",
    "    \"\"\"\n",
    "    A wrapper function to generate anchors given different scales and\n",
    "    ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    shift_x = tf.range(width) * feat_stride # [0,16,32,48] width\n",
    "    shift_y = tf.range(height) * feat_stride # [0,16,32,48] height\n",
    "    shift_x, shift_y = tf.meshgrid(shift_x, shift_y) # meshgrid cols, rows , meshgrid generates a grid of points in ND space\n",
    "    # meshgrid enumerate shift_x row wise and shift_y col wise\n",
    "    shift_x = tf.reshape(shift_x, shape=(-1,)) # reshape to 1D\n",
    "    shift_y = tf.reshape(shift_y, shape=(-1,))\n",
    "    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=1) # vertical stack by row\n",
    "    K = tf.multiply(width, height)\n",
    "    shifts = tf.transpose(tf.reshape(shifts,shape=[1,K,4]),perm=[1,0,2]) # reshaping into Kx1x4\n",
    "\n",
    "    anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales)) # basic 9 anchor boxes of shape (9,4)\n",
    "    A = anchors.shape[0] # 9\n",
    "    anchor_constants = tf.constant(anchors.reshape((1, A, 4)), dtype=tf.int32) # reshape to 1x9x4\n",
    "    \n",
    "    length = K*A\n",
    "    anchors_tf = tf.reshape(tf.add(anchor_constants,shifts),shape=[length,4]) # add shift to anchors element wise\n",
    "    return tf.cast(anchors_tf,tf.float32),length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of tensor_anchors:  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tensor_anchors shape (16650, 4)\n",
      "length=tf.Tensor(16650, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_anchors, length = generate_anchors_pre_tf(height=600//16,width=800//16)\n",
    "print(\"type of tensor_anchors: \",type(tensor_anchors))\n",
    "print(\"tensor_anchors shape\",tensor_anchors.shape)\n",
    "print(\"length=\"+str(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Bounding box regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating bounding box regression coefficients\n",
    "def bbox_transform(original_rois,gt_rois):\n",
    "    original_widths = original_rois[:,2] - original_rois[:,0] + 1.0\n",
    "    original_heights = original_rois[:,3] - original_rois[:,1] + 1.0\n",
    "    original_ctr_x = original_rois[:,0] + 0.5 * original_widths\n",
    "    original_ctr_y = original_rois[:,1] + 0.5 * original_heights\n",
    "\n",
    "    gt_widths = gt_rois[:,2] - gt_rois[:,0] + 1.0\n",
    "    gt_heights = gt_rois[:,3] - gt_rois[:,1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:,0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:,1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - original_ctr_x) / original_widths\n",
    "    targets_dy = (gt_ctr_y - original_ctr_y) / original_heights\n",
    "    targets_dw = np.log(gt_widths / original_widths)\n",
    "    targets_dh = np.log(gt_heights / original_heights)\n",
    "\n",
    "    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05882353, 0.05882353, 0.        , 0.        ],\n",
       "       [0.03030303, 0.03030303, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_transform(np.array([[-1,-1,15,15],[-1,-1,31,31]]),np.array([[0,0,16,16],[0,0,32,32]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform_inv_tf(boxes, deltas):\n",
    "    if boxes.shape[0] == 0:\n",
    "        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n",
    "    \n",
    "    boxes = tf.cast(boxes, deltas.dtype)\n",
    "    Original_widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    Original_heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    Original_ctr_x = boxes[:, 0] + 0.5 * Original_widths\n",
    "    Original_ctr_y = boxes[:, 1] + 0.5 * Original_heights\n",
    "\n",
    "    targets_dx = deltas[:, 0::4]\n",
    "    targets_dy = deltas[:, 1::4]\n",
    "    targets_dw = deltas[:, 2::4]\n",
    "    targets_dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = tf.add(tf.multiply(targets_dx, Original_widths), Original_ctr_x)\n",
    "    pred_ctr_y = tf.add(tf.multiply(targets_dy, Original_heights), Original_ctr_y)\n",
    "    pred_w = tf.multiply(tf.exp(targets_dw), Original_widths)\n",
    "    pred_h = tf.multiply(tf.exp(targets_dh), Original_heights)\n",
    "\n",
    "    pred_boxes0 = tf.subtract(pred_ctr_x,pred_w*0.5)\n",
    "    pred_boxes1 = tf.subtract(pred_ctr_y,pred_h*0.5)\n",
    "    pred_boxes2 = tf.add(pred_ctr_x,pred_w*0.5)\n",
    "    pred_boxes3 = tf.add(pred_ctr_y,pred_h*0.5)\n",
    "\n",
    "    predicted_boxes = tf.stack([pred_boxes0,pred_boxes1,pred_boxes2,pred_boxes3],axis=1)\n",
    "    return predicted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes_tf(boxes, im_info):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    boxes: [N, 4* num_classes]\n",
    "    im_info: [image_height, image_width, scale_ratios]\n",
    "    \"\"\"\n",
    "    # x1 >= 0\n",
    "    boxes[:, 0::4] = tf.maximum(tf.minimum(boxes[:, 0::4], im_info[1] - 1), 0)\n",
    "    # y1 >= 0\n",
    "    boxes[:, 1::4] = tf.maximum(tf.minimum(boxes[:, 1::4], im_info[0] - 1), 0)\n",
    "    # x2 < im_info[1]\n",
    "    boxes[:, 2::4] = tf.maximum(tf.minimum(boxes[:, 2::4], im_info[1] - 1), 0)\n",
    "    # y2 < im_info[0]\n",
    "    boxes[:, 3::4] = tf.maximum(tf.minimum(boxes[:, 3::4], im_info[0] - 1), 0)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS - Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "nms = tf.image.non_max_suppression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_network = VGG16(weights='imagenet', include_top=False, input_shape=(600,800,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 18 layers of vgg16 for head network\n",
    "head_network = Model(inputs=head_network.input, outputs=head_network.get_layer('block5_conv3').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 600, 800, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 600, 800, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 600, 800, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 300, 400, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 300, 400, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 300, 400, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 150, 200, 128)     0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 150, 200, 256)     295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 150, 200, 256)     590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 150, 200, 256)     590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 75, 100, 256)      0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 75, 100, 512)      1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 75, 100, 512)      2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 75, 100, 512)      2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 37, 50, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "head_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_layer(rpn_cls_prob, rpn_bbox_pred, im_info, _feat_stride, anchors, num_anchors):\n",
    "    pre_nms_topN = 12000\n",
    "    post_nms_topN = 2000\n",
    "    nms_thresh = 0.7\n",
    "\n",
    "    scores = rpn_cls_prob[:, :, :, num_anchors:]\n",
    "    scores = tf.reshape(scores, shape=(-1,))\n",
    "    rpn_bbox_pred = tf.reshape(rpn_bbox_pred, shape=(-1, 4))\n",
    "\n",
    "    proposals = bbox_transform_inv_tf(anchors, rpn_bbox_pred)\n",
    "    proposals = clip_boxes_tf(proposals, im_info[:2])\n",
    "\n",
    "    indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)\n",
    "    boxes = tf.gather(proposals, indices)\n",
    "    boxes = tf.to_float(boxes)\n",
    "    scores = tf.gather(scores, indices)\n",
    "    scores = tf.reshape(scores, shape=(-1, 1))\n",
    "\n",
    "    batch_inds = tf.zeros((tf.shape(indices)[0],1), dtype=tf.float32)\n",
    "    rois = tf.concat([batch_inds, boxes], axis=1)\n",
    "\n",
    "    return rois, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_slim as slim\n",
    "\n",
    "initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _region_proposal(net_conv,is_training,initializer):\n",
    "    rpn = slim.conv2d(net_conv,512,[3,3],trainable=is_training,weights_initializer=initializer,scope='rpn_conv/3x3')\n",
    "    rpn_cls_score = slim.conv2d(rpn,num_anchors*2,[1,1],trainable=is_training,weights_initializer=initializer,\n",
    "                                padding='VALID',activation_fn=None,scope='rpn_cls_score')\n",
    "    rpn_bbox_pred = slim.conv2d(rpn,*4,[1,1],trainable=is_training,weights_initializer=initializer,\n",
    "                                padding='VALID',activation_fn=None,scope='rpn_bbox_pred')\n",
    "    rpn_cls_prob = tf.nn.softmax(rpn_cls_score)\n",
    "    rpn_cls_prob = tf.reshape(rpn_cls_prob,[-1,2])\n",
    "    rpn_bbox_pred = tf.reshape(rpn_bbox_pred,[-1,4])\n",
    "    return rpn_cls_prob,rpn_bbox_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Target Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_overlaps(boxes,query_boxes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    boxes: numpy array (N,4)\n",
    "    query_boxes: numpy array (K,4)\n",
    "    Returns:\n",
    "    -------\n",
    "    overlaps: numpy array (N,K)\n",
    "    \"\"\"\n",
    "    N = boxes.shape[0]\n",
    "    K = query_boxes.shape[0]\n",
    "    overlaps = np.zeros((N,K),dtype=np.float)\n",
    "    for k in range(K):\n",
    "        box_area = (\n",
    "            (query_boxes[k, 2] - query_boxes[k, 0] + 1) *\n",
    "            (query_boxes[k, 3] - query_boxes[k, 1] + 1)\n",
    "        )\n",
    "        for n in range(N):\n",
    "            iw = (\n",
    "                min(boxes[n, 2], query_boxes[k, 2]) -\n",
    "                max(boxes[n, 0], query_boxes[k, 0]) + 1\n",
    "            )\n",
    "            if iw > 0:\n",
    "                ih = (\n",
    "                    min(boxes[n, 3], query_boxes[k, 3]) -\n",
    "                    max(boxes[n, 1], query_boxes[k, 1]) + 1\n",
    "                )\n",
    "                if ih > 0:\n",
    "                    ua = float((boxes[n, 2] - boxes[n, 0] + 1) *\n",
    "                           (boxes[n, 3] - boxes[n, 1] + 1) + box_area - iw * ih)\n",
    "                    overlaps[n, k] = iw * ih / ua\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.14285714 0.         1.        ]\n",
      " [0.14285714 1.         0.         0.14285714]\n",
      " [0.         0.         1.         0.        ]]\n",
      "[0 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "## call bbox_overlaps\n",
    "boxes = np.array([[0,0,1,1],[1,1,2,2],[3,3,4,4]])\n",
    "query_boxes = np.array([[0,0,1,1],[1,1,2,2],[3,3,4,4],[0,0,1,1]])\n",
    "overlaps = bbox_overlaps(boxes,query_boxes)\n",
    "print(overlaps)\n",
    "argmax_overlaps = overlaps.argmax(axis=0)\n",
    "print(argmax_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPN_NEGATIVE_OVERLAP = 0.3\n",
    "RPN_POSITIVE_OVERLAP = 0.7\n",
    "RPN_BATCHSIZE = 256\n",
    "RPN_FG_FRACTION = 0.5\n",
    "RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_target_layer(rpn_cls_score,gt_boxes,im_info,_feat_stride,all_anchors,num_anchors):\n",
    "    A = num_anchors\n",
    "    total_anchors = all_anchors.shape[0]\n",
    "    K = total_anchors / num_anchors\n",
    "\n",
    "    # allow boxes to sit over the edge by a small amount\n",
    "    _allowed_border = 0\n",
    "\n",
    "    # map of shape (..., H, W)\n",
    "    height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "    # only keep anchors inside the image\n",
    "    inds_inside = np.where(\n",
    "        (all_anchors[:, 0] >= -_allowed_border) &\n",
    "        (all_anchors[:, 1] >= -_allowed_border) &\n",
    "        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n",
    "        (all_anchors[:, 3] < im_info[0] + _allowed_border)  # height\n",
    "    )[0]\n",
    "\n",
    "    # keep only inside anchors\n",
    "    anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "    # label: 1 is positive, 0 is negative, -1 is dont care\n",
    "    labels = np.empty((len(inds_inside),), dtype=np.float32)\n",
    "    labels.fill(-1)\n",
    "\n",
    "    # overlaps between the anchors and the gt boxes\n",
    "    # overlaps (ex, gt)\n",
    "    overlaps = bbox_overlaps(\n",
    "        np.ascontiguousarray(anchors, dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes, dtype=np.float))\n",
    "    argmax_overlaps = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "    gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n",
    "                                 np.arange(overlaps.shape[1])]\n",
    "    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "    labels[max_overlaps < RPN_NEGATIVE_OVERLAP] = 0\n",
    "    labels[gt_argmax_overlaps] = 1\n",
    "    labels[max_overlaps >= RPN_POSITIVE_OVERLAP] = 1\n",
    "\n",
    "    # subsample positive labels if we have too many\n",
    "    num_fg = int(RPN_FG_FRACTION * RPN_BATCHSIZE)\n",
    "    fg_inds = np.where(labels == 1)[0]\n",
    "    if len(fg_inds) > num_fg:\n",
    "        disable_inds = npr.choice(\n",
    "            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
    "        labels[disable_inds] = -1\n",
    "    \n",
    "    # subsample negative labels if we have too many\n",
    "    num_bg = RPN_BATCHSIZE - np.sum(labels == 1)\n",
    "    bg_inds = np.where(labels == 0)[0]\n",
    "    if len(bg_inds) > num_bg:\n",
    "        disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "        labels[disable_inds] = -1\n",
    "    \n",
    "    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n",
    "\n",
    "    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    bbox_inside_weights[labels == 1, :] = np.array(RPN_BBOX_INSIDE_WEIGHTS)\n",
    "\n",
    "    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    \n",
    "    bbox_outside_weights[labels == 1, :] = 1.0\n",
    "    bbox_outside_weights[labels == 0, :] = 1.0\n",
    "\n",
    "    # map up to original set of anchors\n",
    "    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n",
    "    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n",
    "    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n",
    "\n",
    "    # labels\n",
    "    labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n",
    "    labels = labels.reshape((1, 1, A * height, width))\n",
    "    rpn_labels = labels\n",
    "\n",
    "    # bbox_targets\n",
    "    bbox_targets = bbox_targets.reshape((1, height, width, A * 4))\n",
    "    \n",
    "    rpn_bbox_targets = bbox_targets\n",
    "\n",
    "    # bbox_inside_weights\n",
    "    bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, A * 4))\n",
    "    rpn_bbox_inside_weights = bbox_inside_weights\n",
    "\n",
    "    # bbox_outside_weights\n",
    "    bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, A * 4))\n",
    "    rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "    return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unmap(data, count, inds, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of size count) \"\"\"\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_targets(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    assert ex_rois.shape[0] == gt_rois.shape[0]\n",
    "    assert ex_rois.shape[1] == 4\n",
    "    assert gt_rois.shape[1] == 5\n",
    "\n",
    "    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call _compute_targets()\n",
    "anchors = np.array([[0, 0, 10, 10],[0, 0, 20, 20],[0, 0, 30, 30]])\n",
    "gt_boxes = np.array([[0, 0, 10, 10, 1],[0, 0, 20, 20, 1],[0, 0, 30, 30, 1]])\n",
    "_compute_targets(anchors, gt_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal Target Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "FG_FRACTION = 0.25\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "FG_THRESH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_target_layer(rpn_rois,rpn_score,gt_boxes,_num_classes):\n",
    "    \"\"\"\n",
    "    Assign object detection proposals to ground-truth targets. Produces proposal\n",
    "    classification labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "    all_rois = rpn_rois\n",
    "    all_scores = rpn_score\n",
    "\n",
    "    # Include ground-truth boxes in the set of candidate rois\n",
    "    zeros = np.zeros((gt_boxes.shape[0], 1), dtype=gt_boxes.dtype)\n",
    "    all_rois = np.vstack(\n",
    "        (all_rois, np.hstack((zeros, gt_boxes[:, :-1])))\n",
    "    )\n",
    "    all_scores = np.vstack((all_scores, zeros))\n",
    "\n",
    "    num_images = 1\n",
    "    rois_per_image = BATCH_SIZE / num_images\n",
    "    fg_rois_per_image = np.round(FG_FRACTION * rois_per_image)\n",
    "\n",
    "    # Sample rois with classification labels and bounding box regression targets\n",
    "    labels, rois, roi_scores, bbox_targets, bbox_inside_weights = _sample_rois(\n",
    "        all_rois, all_scores, gt_boxes, fg_rois_per_image, rois_per_image, _num_classes)\n",
    "    \n",
    "    rois = rois.reshape(-1, 5)\n",
    "    roi_scores = roi_scores.reshape(-1)\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    bbox_targets = bbox_targets.reshape(-1, _num_classes * 4)\n",
    "    bbox_inside_weights = bbox_inside_weights.reshape(-1, _num_classes * 4)\n",
    "    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n",
    "\n",
    "    return rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bbox_regression_labels(bbox_target_data,num_classes):\n",
    "    \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "    compact form N x (class, tx, ty, tw, th)\n",
    "\n",
    "    This function expands those targets into the 4-of-4*K representation used\n",
    "    by the network (i.e. only one class has non-zero targets).\n",
    "\n",
    "    Returns:\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "    \"\"\"\n",
    "\n",
    "    clss = bbox_target_data[:, 0]\n",
    "    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n",
    "    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n",
    "    inds = np.where(clss > 0)[0]\n",
    "    for ind in inds:\n",
    "        cls = clss[ind]\n",
    "        start = int(4 * cls)\n",
    "        end = start + 4\n",
    "        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n",
    "        bbox_inside_weights[ind, start:end] = BBOX_INSIDE_WEIGHTS\n",
    "    return bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_targets_PTL(ex_rois,gt_rois,labels):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    assert ex_rois.shape[0] == gt_rois.shape[0]\n",
    "    assert ex_rois.shape[1] == 4\n",
    "    assert gt_rois.shape[1] == 4\n",
    "\n",
    "    targets = bbox_transform(ex_rois, gt_rois).astype(np.float32, copy=False)\n",
    "    targets = np.hstack((labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20., 20.,  0.,\n",
       "          0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 30., 30.,  0.,\n",
       "          0.,  0.,  0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call _get_bbox_regression_labels function\n",
    "bbox_target_data = np.array([[1,0,0,10,10],[2,0,0,20,20],[2,0,0,30,30]])\n",
    "num_classes = 4\n",
    "_get_bbox_regression_labels(bbox_target_data,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_rois(all_rois, all_scores, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n",
    "    \"\"\" Generate a random sample of RoIs comprising foreground and background examples\n",
    "    \"\"\"\n",
    "    # overlaps: (rois x gt_boxes)\n",
    "    overlaps = bbox_overlaps(\n",
    "        np.ascontiguousarray(all_rois[:,1:5],dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes[:,:4],dtype=np.float))\n",
    "    gt_assignment = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps.max(axis=1)\n",
    "    labels = gt_boxes[gt_assignment, 4]\n",
    "\n",
    "    # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "    fg_inds = np.where(max_overlaps >= FG_THRESH)[0]\n",
    "    # Select background RoIs as those within [BG_THRESH_LO,GB_THRESH_HI)\n",
    "    bg_inds = np.where((max_overlaps < BG_THRESH_HI) &\n",
    "                          (max_overlaps >= BG_THRESH_LO))[0]\n",
    "\n",
    "    # Ensure that a fixed number of regions are sampled\n",
    "    if fg_inds.size > 0 and bg_inds.size > 0:\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.size)\n",
    "        fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_image), replace=False)\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "        to_replace = bg_inds.size < bg_rois_per_image\n",
    "        bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_image), replace=to_replace)\n",
    "    elif fg_inds > 0:\n",
    "        to_replace = fg_inds.size < rois_per_image\n",
    "        fg_inds = npr.choice(fg_inds, size=int(rois_per_image), replace=to_replace)\n",
    "        fg_rois_per_image = rois_per_image\n",
    "    elif bg_inds > 0:\n",
    "        to_repace = bg_inds.size < rois_per_image\n",
    "        bg_inds = npr.choice(bg_inds, size=int(rois_per_image), replace=to_replace)\n",
    "        fg_rois_per_image = 0\n",
    "    \n",
    "    # the indices that we are selecting (both fg and bg)\n",
    "    keep_inds = np.append(fg_inds,bg_inds)\n",
    "    # Select sampled values from various arrays:\n",
    "    labels = labels[keep_inds]\n",
    "    # Clamp labels for the background RoIs to 0\n",
    "    labels[int(fg_rois_per_image):] = 0\n",
    "    rois = all_rois[keep_inds]\n",
    "    rois_scores = all_scores[keep_inds]\n",
    "\n",
    "    bbox_target_data = _compute_targets_PTL(\n",
    "        rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4],labels)\n",
    "    \n",
    "    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(bbox_target_data,num_classes)\n",
    "\n",
    "    return labels, rois, rois_scores, bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Pooling Layer\n",
    "will implement this layer directly in Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import range\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "import PIL.ImageColor as ImageColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]\n",
    "\n",
    "NUM_COLORS = len(STANDARD_COLORS)\n",
    "\n",
    "try:\n",
    "    FONT = ImageFont.truetype('arial.ttf', 24)\n",
    "except IOError:\n",
    "    FONT = ImageFont.load_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_single_box(image, xmin, ymin, xmax, ymax, display_str, font, color='black', thickness=4):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "                (right, top), (left, top)], width=thickness, fill=color)\n",
    "    text_bottom = bottom\n",
    "    # Reverse list and print from bottom to top.\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin), display_str, fill='black', font=font)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(image,gt_boxes,im_info):\n",
    "    num_boxes = gt_boxes.shape[0]\n",
    "    gt_boxes_new = gt_boxes.copy()\n",
    "    gt_boxes_new[:,:4] = np.round(gt_boxes_new[:,:4]/copy() / im_info[2])\n",
    "    disp_image = Image.fromarray(np.uint8(image[0]))\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        this_class = int(gt_boxes_new[i,4])\n",
    "        disp_image = _draw_single_box(disp_image,gt_boxes_new[i,0],gt_boxes_new[i,1],gt_boxes_new[i,2],gt_boxes_new[i,3],'N%02d-C%02d'%(i,this_class),FONT,color=STANDARD_COLORS[this_class%NUM_COLORS])\n",
    "        image[0,:] = np.array(disp_image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578ce72e72bd9f13049fd4c13d9f5b1c81715c13ddea0a3c61ff70756cb5d6d4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
