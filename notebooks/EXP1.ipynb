{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Generation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_cor(anchor):\n",
    "    \"\"\"\n",
    "    Return width, height, x center, and y center for an anchor (window).\n",
    "    \"\"\"\n",
    "    # anchor is 1X4\n",
    "    w = anchor[2] - anchor[0] + 1\n",
    "    h = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + 0.5 * (w - 1)\n",
    "    y_ctr = anchor[1] + 0.5*(w-1)\n",
    "    return w,h,x_ctr,y_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_anchors(ws,hs,x_ctr,y_ctr):\n",
    "    \"\"\"\n",
    "    Given a vector of widths (ws) and heights (hs) around a center (x_ctr,y_ctr),\n",
    "    Return a set of anchor boxes in (w, h, x_ctr, y_ctr) format.\n",
    "    \"\"\"\n",
    "    # ws,hs,x_ctr,y_ctr are numpy arrays\n",
    "    w = ws[:,np.newaxis] # [1,4] -> [4,1] i.e it generate array of one more dimension\n",
    "    # print('w in mkanchors: ',w)\n",
    "    h = hs[:,np.newaxis] # [1,4] -> [4,1]\n",
    "    # print('h in mkanchors: ',h)\n",
    "    anchors = np.hstack((x_ctr - 0.5*(w-1),\n",
    "                                            y_ctr - 0.5*(h-1),\n",
    "                                            x_ctr + 0.5*(w-1),\n",
    "                                            y_ctr + 0.5*(h-1))) # horizontal stack\n",
    "    # print('anchors in mkanchors: ',anchors)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ratio_enum(anchor,ratios):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n",
    "    \"\"\"\n",
    "    \n",
    "    w,h,x_ctr,y_ctr = _extract_cor(anchor)\n",
    "    # print(w,h,x_ctr,y_ctr)\n",
    "    size = w*h\n",
    "    # print('size='+str(size))\n",
    "    size_ratios = size/ratios\n",
    "    # print('size_ratios='+str(size_ratios))\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    # print('ws='+str(ws))\n",
    "    hs = np.round(ws*ratios)\n",
    "    # print('hs='+str(hs))\n",
    "    anchors = _make_anchors(ws,hs,x_ctr,y_ctr)\n",
    "    # print('anchors in ratio_enu='+str(anchors))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_enum(anchor,scales):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each scale wrt an anchor.\n",
    "    \"\"\"\n",
    "    w, h, x_ctr, y_ctr = _extract_cor(anchor)\n",
    "    # print(\"anchor in scle_enum=\"+str(anchor))\n",
    "    ws = w*scales\n",
    "    hs = h*scales\n",
    "    anchors = _make_anchors(ws,hs,x_ctr,y_ctr)\n",
    "    # print(\"Final anchors in scale_enum=\"+str(anchors))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  It generates 9 anchor boxes from a base anchor box\n",
    "\n",
    "def generate_anchors(base_size=16,ratios=[0.5,1,2],scales=np.array([8,16,32])):\n",
    "    base_anchor = np.array([1,1,base_size,base_size]) - 1\n",
    "    # print(base_anchor)\n",
    "    ratio_anchors = _ratio_enum(base_anchor,ratios)\n",
    "    # print(ratio_anchors.shape)\n",
    "    anchors_list=[]\n",
    "    for i in range(ratio_anchors.shape[0]):\n",
    "        anc = _scale_enum(ratio_anchors[i,:],scales)\n",
    "        anchors_list.append(anc)\n",
    "    anchors = np.vstack(anchors_list)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -84. ,  -34.5,   99. ,   60.5],\n",
       "       [-176. ,  -82.5,  191. ,  108.5],\n",
       "       [-360. , -178.5,  375. ,  204.5],\n",
       "       [ -56. ,  -56. ,   71. ,   71. ],\n",
       "       [-120. , -120. ,  135. ,  135. ],\n",
       "       [-248. , -248. ,  263. ,  263. ],\n",
       "       [ -36. ,  -85.5,   51. ,   89.5],\n",
       "       [ -80. , -173.5,   95. ,  177.5],\n",
       "       [-168. , -349.5,  183. ,  353.5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create uniformly spaced grid with spacing equal to stride\n",
    "\n",
    "def generate_anchors_pre_tf(height, width, feat_stride=16, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n",
    "    \"\"\"\n",
    "    A wrapper function to generate anchors given different scales and\n",
    "    ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    shift_x = tf.range(width) * feat_stride # [0,16,32,48] width\n",
    "    shift_y = tf.range(height) * feat_stride # [0,16,32,48] height\n",
    "    shift_x, shift_y = tf.meshgrid(shift_x, shift_y) # meshgrid cols, rows , meshgrid generates a grid of points in ND space\n",
    "    # meshgrid enumerate shift_x row wise and shift_y col wise\n",
    "    shift_x = tf.reshape(shift_x, shape=(-1,)) # reshape to 1D\n",
    "    shift_y = tf.reshape(shift_y, shape=(-1,))\n",
    "    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=1) # vertical stack by row\n",
    "    K = tf.multiply(width, height)\n",
    "    shifts = tf.transpose(tf.reshape(shifts,shape=[1,K,4]),perm=[1,0,2]) # reshaping into Kx1x4\n",
    "\n",
    "    anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales)) # basic 9 anchor boxes of shape (9,4)\n",
    "    A = anchors.shape[0] # 9\n",
    "    anchor_constants = tf.constant(anchors.reshape((1, A, 4)), dtype=tf.int32) # reshape to 1x9x4\n",
    "    \n",
    "    length = K*A\n",
    "    anchors_tf = tf.reshape(tf.add(anchor_constants,shifts),shape=[length,4]) # add shift to anchors element wise\n",
    "    return tf.cast(anchors_tf,tf.float32),length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of tensor_anchors:  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tensor_anchors shape (16650, 4)\n",
      "length=tf.Tensor(16650, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor_anchors, length = generate_anchors_pre_tf(height=600//16,width=800//16)\n",
    "print(\"type of tensor_anchors: \",type(tensor_anchors))\n",
    "print(\"tensor_anchors shape\",tensor_anchors.shape)\n",
    "print(\"length=\"+str(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Bounding box regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating bounding box regression coefficients\n",
    "def bbox_transform(original_rois,gt_rois):\n",
    "    original_widths = original_rois[:,2] - original_rois[:,0] + 1.0\n",
    "    original_heights = original_rois[:,3] - original_rois[:,1] + 1.0\n",
    "    original_ctr_x = original_rois[:,0] + 0.5 * original_widths\n",
    "    original_ctr_y = original_rois[:,1] + 0.5 * original_heights\n",
    "\n",
    "    gt_widths = gt_rois[:,2] - gt_rois[:,0] + 1.0\n",
    "    gt_heights = gt_rois[:,3] - gt_rois[:,1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:,0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:,1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - original_ctr_x) / original_widths\n",
    "    targets_dy = (gt_ctr_y - original_ctr_y) / original_heights\n",
    "    targets_dw = np.log(gt_widths / original_widths)\n",
    "    targets_dh = np.log(gt_heights / original_heights)\n",
    "\n",
    "    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05882353, 0.05882353, 0.        , 0.        ],\n",
       "       [0.03030303, 0.03030303, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_transform(np.array([[-1,-1,15,15],[-1,-1,31,31]]),np.array([[0,0,16,16],[0,0,32,32]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform_inv_tf(boxes, deltas):\n",
    "    if boxes.shape[0] == 0:\n",
    "        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n",
    "    \n",
    "    boxes = tf.cast(boxes, deltas.dtype)\n",
    "    Original_widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    Original_heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    Original_ctr_x = boxes[:, 0] + 0.5 * Original_widths\n",
    "    Original_ctr_y = boxes[:, 1] + 0.5 * Original_heights\n",
    "\n",
    "    targets_dx = deltas[:, 0::4]\n",
    "    targets_dy = deltas[:, 1::4]\n",
    "    targets_dw = deltas[:, 2::4]\n",
    "    targets_dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = tf.add(tf.multiply(targets_dx, Original_widths), Original_ctr_x)\n",
    "    pred_ctr_y = tf.add(tf.multiply(targets_dy, Original_heights), Original_ctr_y)\n",
    "    pred_w = tf.multiply(tf.exp(targets_dw), Original_widths)\n",
    "    pred_h = tf.multiply(tf.exp(targets_dh), Original_heights)\n",
    "\n",
    "    pred_boxes0 = tf.subtract(pred_ctr_x,pred_w*0.5)\n",
    "    pred_boxes1 = tf.subtract(pred_ctr_y,pred_h*0.5)\n",
    "    pred_boxes2 = tf.add(pred_ctr_x,pred_w*0.5)\n",
    "    pred_boxes3 = tf.add(pred_ctr_y,pred_h*0.5)\n",
    "\n",
    "    predicted_boxes = tf.stack([pred_boxes0,pred_boxes1,pred_boxes2,pred_boxes3],axis=1)\n",
    "    return predicted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes_tf(boxes, im_info):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    boxes: [N, 4* num_classes]\n",
    "    im_info: [image_height, image_width, scale_ratios]\n",
    "    \"\"\"\n",
    "    # x1 >= 0\n",
    "    boxes[:, 0::4] = tf.maximum(tf.minimum(boxes[:, 0::4], im_info[1] - 1), 0)\n",
    "    # y1 >= 0\n",
    "    boxes[:, 1::4] = tf.maximum(tf.minimum(boxes[:, 1::4], im_info[0] - 1), 0)\n",
    "    # x2 < im_info[1]\n",
    "    boxes[:, 2::4] = tf.maximum(tf.minimum(boxes[:, 2::4], im_info[1] - 1), 0)\n",
    "    # y2 < im_info[0]\n",
    "    boxes[:, 3::4] = tf.maximum(tf.minimum(boxes[:, 3::4], im_info[0] - 1), 0)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS - Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_network = VGG16(weights='imagenet', include_top=False, input_shape=(600,800,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 18 layers of vgg16 for head network\n",
    "head_network = Model(inputs=head_network.input, outputs=head_network.get_layer('block5_conv3').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 600, 800, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 600, 800, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 600, 800, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 300, 400, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 300, 400, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 300, 400, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 150, 200, 128)     0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 150, 200, 256)     295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 150, 200, 256)     590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 150, 200, 256)     590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 75, 100, 256)      0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 75, 100, 512)      1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 75, 100, 512)      2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 75, 100, 512)      2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 37, 50, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 37, 50, 512)       2359808   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "head_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_layer(rpn_cls_prob, rpn_bbox_pred, im_info, _feat_stride, anchors, num_anchors):\n",
    "    pre_nms_topN = 12000\n",
    "    post_nms_topN = 2000\n",
    "    nms_thresh = 0.7\n",
    "\n",
    "    scores = rpn_cls_prob[:, :, :, num_anchors:]\n",
    "    scores = tf.reshape(scores, shape=(-1,))\n",
    "    rpn_bbox_pred = tf.reshape(rpn_bbox_pred, shape=(-1, 4))\n",
    "\n",
    "    proposals = bbox_transform_inv_tf(anchors, rpn_bbox_pred)\n",
    "    proposals = clip_boxes_tf(proposals, im_info[:2])\n",
    "\n",
    "    indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)\n",
    "    boxes = tf.gather(proposals, indices)\n",
    "    boxes = tf.to_float(boxes)\n",
    "    scores = tf.gather(scores, indices)\n",
    "    scores = tf.reshape(scores, shape=(-1, 1))\n",
    "\n",
    "    batch_inds = tf.zeros((tf.shape(indices)[0],1), dtype=tf.float32)\n",
    "    rois = tf.concat([batch_inds, boxes], axis=1)\n",
    "\n",
    "    return rois, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_slim as slim\n",
    "\n",
    "initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _region_proposal(net_conv,is_training,initializer):\n",
    "    rpn = slim.conv2d(net_conv,512,[3,3],trainable=is_training,weights_initializer=initializer,scope='rpn_conv/3x3')\n",
    "    rpn_cls_score = slim.conv2d(rpn,num_anchors*2,[1,1],trainable=is_training,weights_initializer=initializer,\n",
    "                                padding='VALID',activation_fn=None,scope='rpn_cls_score')\n",
    "    rpn_bbox_pred = slim.conv2d(rpn,*4,[1,1],trainable=is_training,weights_initializer=initializer,\n",
    "                                padding='VALID',activation_fn=None,scope='rpn_bbox_pred')\n",
    "    rpn_cls_prob = tf.nn.softmax(rpn_cls_score)\n",
    "    rpn_cls_prob = tf.reshape(rpn_cls_prob,[-1,2])\n",
    "    rpn_bbox_pred = tf.reshape(rpn_bbox_pred,[-1,4])\n",
    "    return rpn_cls_prob,rpn_bbox_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Target Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_overlaps(boxes,query_boxes):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    boxes: numpy array (N,4)\n",
    "    query_boxes: numpy array (K,4)\n",
    "    Returns:\n",
    "    -------\n",
    "    overlaps: numpy array (N,K)\n",
    "    \"\"\"\n",
    "    N = boxes.shape[0]\n",
    "    K = query_boxes.shape[0]\n",
    "    overlaps = np.zeros((N,K),dtype=np.float)\n",
    "    for k in range(K):\n",
    "        box_area = (\n",
    "            (query_boxes[k, 2] - query_boxes[k, 0] + 1) *\n",
    "            (query_boxes[k, 3] - query_boxes[k, 1] + 1)\n",
    "        )\n",
    "        for n in range(N):\n",
    "            iw = (\n",
    "                min(boxes[n, 2], query_boxes[k, 2]) -\n",
    "                max(boxes[n, 0], query_boxes[k, 0]) + 1\n",
    "            )\n",
    "            if iw > 0:\n",
    "                ih = (\n",
    "                    min(boxes[n, 3], query_boxes[k, 3]) -\n",
    "                    max(boxes[n, 1], query_boxes[k, 1]) + 1\n",
    "                )\n",
    "                if ih > 0:\n",
    "                    ua = float((boxes[n, 2] - boxes[n, 0] + 1) *\n",
    "                           (boxes[n, 3] - boxes[n, 1] + 1) + box_area - iw * ih)\n",
    "                    overlaps[n, k] = iw * ih / ua\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.14285714 0.         1.        ]\n",
      " [0.14285714 1.         0.         0.14285714]\n",
      " [0.         0.         1.         0.        ]]\n",
      "[0 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "## call bbox_overlaps\n",
    "boxes = np.array([[0,0,1,1],[1,1,2,2],[3,3,4,4]])\n",
    "query_boxes = np.array([[0,0,1,1],[1,1,2,2],[3,3,4,4],[0,0,1,1]])\n",
    "overlaps = bbox_overlaps(boxes,query_boxes)\n",
    "print(overlaps)\n",
    "argmax_overlaps = overlaps.argmax(axis=0)\n",
    "print(argmax_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPN_NEGATIVE_OVERLAP = 0.3\n",
    "RPN_POSITIVE_OVERLAP = 0.7\n",
    "RPN_BATCHSIZE = 256\n",
    "RPN_FG_FRACTION = 0.5\n",
    "RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_target_layer(rpn_cls_score,gt_boxes,im_info,_feat_stride,all_anchors,num_anchors):\n",
    "    A = num_anchors\n",
    "    total_anchors = all_anchors.shape[0]\n",
    "    K = total_anchors / num_anchors\n",
    "\n",
    "    # allow boxes to sit over the edge by a small amount\n",
    "    _allowed_border = 0\n",
    "\n",
    "    # map of shape (..., H, W)\n",
    "    height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "    # only keep anchors inside the image\n",
    "    inds_inside = np.where(\n",
    "        (all_anchors[:, 0] >= -_allowed_border) &\n",
    "        (all_anchors[:, 1] >= -_allowed_border) &\n",
    "        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n",
    "        (all_anchors[:, 3] < im_info[0] + _allowed_border)  # height\n",
    "    )[0]\n",
    "\n",
    "    # keep only inside anchors\n",
    "    anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "    # label: 1 is positive, 0 is negative, -1 is dont care\n",
    "    labels = np.empty((len(inds_inside),), dtype=np.float32)\n",
    "    labels.fill(-1)\n",
    "\n",
    "    # overlaps between the anchors and the gt boxes\n",
    "    # overlaps (ex, gt)\n",
    "    overlaps = bbox_overlaps(\n",
    "        np.ascontiguousarray(anchors, dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes, dtype=np.float))\n",
    "    argmax_overlaps = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "    gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n",
    "                                 np.arange(overlaps.shape[1])]\n",
    "    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "    labels[max_overlaps < RPN_NEGATIVE_OVERLAP] = 0\n",
    "    labels[gt_argmax_overlaps] = 1\n",
    "    labels[max_overlaps >= RPN_POSITIVE_OVERLAP] = 1\n",
    "\n",
    "    # subsample positive labels if we have too many\n",
    "    num_fg = int(RPN_FG_FRACTION * RPN_BATCHSIZE)\n",
    "    fg_inds = np.where(labels == 1)[0]\n",
    "    if len(fg_inds) > num_fg:\n",
    "        disable_inds = npr.choice(\n",
    "            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
    "        labels[disable_inds] = -1\n",
    "    \n",
    "    # subsample negative labels if we have too many\n",
    "    num_bg = RPN_BATCHSIZE - np.sum(labels == 1)\n",
    "    bg_inds = np.where(labels == 0)[0]\n",
    "    if len(bg_inds) > num_bg:\n",
    "        disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "        labels[disable_inds] = -1\n",
    "    \n",
    "    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n",
    "\n",
    "    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    bbox_inside_weights[labels == 1, :] = np.array(RPN_BBOX_INSIDE_WEIGHTS)\n",
    "\n",
    "    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "    \n",
    "    bbox_outside_weights[labels == 1, :] = 1.0\n",
    "    bbox_outside_weights[labels == 0, :] = 1.0\n",
    "\n",
    "    # map up to original set of anchors\n",
    "    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n",
    "    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n",
    "    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n",
    "\n",
    "    # labels\n",
    "    labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n",
    "    labels = labels.reshape((1, 1, A * height, width))\n",
    "    rpn_labels = labels\n",
    "\n",
    "    # bbox_targets\n",
    "    bbox_targets = bbox_targets.reshape((1, height, width, A * 4))\n",
    "    \n",
    "    rpn_bbox_targets = bbox_targets\n",
    "\n",
    "    # bbox_inside_weights\n",
    "    bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, A * 4))\n",
    "    rpn_bbox_inside_weights = bbox_inside_weights\n",
    "\n",
    "    # bbox_outside_weights\n",
    "    bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, A * 4))\n",
    "    rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "    return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unmap(data, count, inds, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of size count) \"\"\"\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_targets(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    assert ex_rois.shape[0] == gt_rois.shape[0]\n",
    "    assert ex_rois.shape[1] == 4\n",
    "    assert gt_rois.shape[1] == 5\n",
    "\n",
    "    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call _compute_targets()\n",
    "anchors = np.array([[0, 0, 10, 10],[0, 0, 20, 20],[0, 0, 30, 30]])\n",
    "gt_boxes = np.array([[0, 0, 10, 10, 1],[0, 0, 20, 20, 1],[0, 0, 30, 30, 1]])\n",
    "_compute_targets(anchors, gt_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal Target Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "FG_FRACTION = 0.25\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "FG_THRESH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_target_layer(rpn_rois,rpn_score,gt_boxes,_num_classes):\n",
    "    \"\"\"\n",
    "    Assign object detection proposals to ground-truth targets. Produces proposal\n",
    "    classification labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "    all_rois = rpn_rois\n",
    "    all_scores = rpn_score\n",
    "\n",
    "    # Include ground-truth boxes in the set of candidate rois\n",
    "    zeros = np.zeros((gt_boxes.shape[0], 1), dtype=gt_boxes.dtype)\n",
    "    all_rois = np.vstack(\n",
    "        (all_rois, np.hstack((zeros, gt_boxes[:, :-1])))\n",
    "    )\n",
    "    all_scores = np.vstack((all_scores, zeros))\n",
    "\n",
    "    num_images = 1\n",
    "    rois_per_image = BATCH_SIZE / num_images\n",
    "    fg_rois_per_image = np.round(FG_FRACTION * rois_per_image)\n",
    "\n",
    "    # Sample rois with classification labels and bounding box regression targets\n",
    "    labels, rois, roi_scores, bbox_targets, bbox_inside_weights = _sample_rois(\n",
    "        all_rois, all_scores, gt_boxes, fg_rois_per_image, rois_per_image, _num_classes)\n",
    "    \n",
    "    rois = rois.reshape(-1, 5)\n",
    "    roi_scores = roi_scores.reshape(-1)\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    bbox_targets = bbox_targets.reshape(-1, _num_classes * 4)\n",
    "    bbox_inside_weights = bbox_inside_weights.reshape(-1, _num_classes * 4)\n",
    "    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n",
    "\n",
    "    return rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bbox_regression_labels(bbox_target_data,num_classes):\n",
    "    \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "    compact form N x (class, tx, ty, tw, th)\n",
    "\n",
    "    This function expands those targets into the 4-of-4*K representation used\n",
    "    by the network (i.e. only one class has non-zero targets).\n",
    "\n",
    "    Returns:\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "    \"\"\"\n",
    "\n",
    "    clss = bbox_target_data[:, 0]\n",
    "    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n",
    "    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n",
    "    inds = np.where(clss > 0)[0]\n",
    "    for ind in inds:\n",
    "        cls = clss[ind]\n",
    "        start = int(4 * cls)\n",
    "        end = start + 4\n",
    "        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n",
    "        bbox_inside_weights[ind, start:end] = BBOX_INSIDE_WEIGHTS\n",
    "    return bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_targets_PTL(ex_rois,gt_rois,labels):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    assert ex_rois.shape[0] == gt_rois.shape[0]\n",
    "    assert ex_rois.shape[1] == 4\n",
    "    assert gt_rois.shape[1] == 4\n",
    "\n",
    "    targets = bbox_transform(ex_rois, gt_rois).astype(np.float32, copy=False)\n",
    "    targets = np.hstack((labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 20., 20.,  0.,\n",
       "          0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 30., 30.,  0.,\n",
       "          0.,  0.,  0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call _get_bbox_regression_labels function\n",
    "bbox_target_data = np.array([[1,0,0,10,10],[2,0,0,20,20],[2,0,0,30,30]])\n",
    "num_classes = 4\n",
    "_get_bbox_regression_labels(bbox_target_data,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_rois(all_rois, all_scores, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n",
    "    \"\"\" Generate a random sample of RoIs comprising foreground and background examples\n",
    "    \"\"\"\n",
    "    # overlaps: (rois x gt_boxes)\n",
    "    overlaps = bbox_overlaps(\n",
    "        np.ascontiguousarray(all_rois[:,1:5],dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes[:,:4],dtype=np.float))\n",
    "    gt_assignment = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps.max(axis=1)\n",
    "    labels = gt_boxes[gt_assignment, 4]\n",
    "\n",
    "    # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "    fg_inds = np.where(max_overlaps >= FG_THRESH)[0]\n",
    "    # Select background RoIs as those within [BG_THRESH_LO,GB_THRESH_HI)\n",
    "    bg_inds = np.where((max_overlaps < BG_THRESH_HI) &\n",
    "                          (max_overlaps >= BG_THRESH_LO))[0]\n",
    "\n",
    "    # Ensure that a fixed number of regions are sampled\n",
    "    if fg_inds.size > 0 and bg_inds.size > 0:\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.size)\n",
    "        fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_image), replace=False)\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "        to_replace = bg_inds.size < bg_rois_per_image\n",
    "        bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_image), replace=to_replace)\n",
    "    elif fg_inds > 0:\n",
    "        to_replace = fg_inds.size < rois_per_image\n",
    "        fg_inds = npr.choice(fg_inds, size=int(rois_per_image), replace=to_replace)\n",
    "        fg_rois_per_image = rois_per_image\n",
    "    elif bg_inds > 0:\n",
    "        to_repace = bg_inds.size < rois_per_image\n",
    "        bg_inds = npr.choice(bg_inds, size=int(rois_per_image), replace=to_replace)\n",
    "        fg_rois_per_image = 0\n",
    "    \n",
    "    # the indices that we are selecting (both fg and bg)\n",
    "    keep_inds = np.append(fg_inds,bg_inds)\n",
    "    # Select sampled values from various arrays:\n",
    "    labels = labels[keep_inds]\n",
    "    # Clamp labels for the background RoIs to 0\n",
    "    labels[int(fg_rois_per_image):] = 0\n",
    "    rois = all_rois[keep_inds]\n",
    "    rois_scores = all_scores[keep_inds]\n",
    "\n",
    "    bbox_target_data = _compute_targets_PTL(\n",
    "        rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4],labels)\n",
    "    \n",
    "    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(bbox_target_data,num_classes)\n",
    "\n",
    "    return labels, rois, rois_scores, bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Pooling Layer\n",
    "will implement this layer directly in Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import range\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "import PIL.ImageColor as ImageColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]\n",
    "\n",
    "NUM_COLORS = len(STANDARD_COLORS)\n",
    "\n",
    "try:\n",
    "    FONT = ImageFont.truetype('arial.ttf', 24)\n",
    "except IOError:\n",
    "    FONT = ImageFont.load_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_single_box(image, xmin, ymin, xmax, ymax, display_str, font, color='black', thickness=4):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "                (right, top), (left, top)], width=thickness, fill=color)\n",
    "    text_bottom = bottom\n",
    "    # Reverse list and print from bottom to top.\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin), display_str, fill='black', font=font)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(image,gt_boxes,im_info):\n",
    "    num_boxes = gt_boxes.shape[0]\n",
    "    gt_boxes_new = gt_boxes.copy()\n",
    "    gt_boxes_new[:,:4] = np.round(gt_boxes_new[:,:4]/copy() / im_info[2])\n",
    "    disp_image = Image.fromarray(np.uint8(image[0]))\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        this_class = int(gt_boxes_new[i,4])\n",
    "        disp_image = _draw_single_box(disp_image,gt_boxes_new[i,0],gt_boxes_new[i,1],gt_boxes_new[i,2],gt_boxes_new[i,3],'N%02d-C%02d'%(i,this_class),FONT,color=STANDARD_COLORS[this_class%NUM_COLORS])\n",
    "        image[0,:] = np.array(disp_image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets={}\n",
    "        self._layers = {}\n",
    "        self._gt_image = None\n",
    "        self._act_summaries = []\n",
    "        self._score_summaries = {}\n",
    "        self._train_summaries = []\n",
    "        self._event_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "\n",
    "    def _add_gt_image(self):\n",
    "        # add back mean\n",
    "        image - self._image + np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "        # BGR to RGB\n",
    "        resize = tf.image.resize(resized,self._im_info[:2],method=tf.image.ResizeMethod.BILINEAR)\n",
    "        self._gt_image = tf.reverse(resize,axis=[-1])\n",
    "    \n",
    "    def _add_gt_image_summary(self):\n",
    "        # use a customized visualization function to visualize the boxes\n",
    "        if self._gt_image is None:\n",
    "            self._add_gt_image()\n",
    "        image = tf.py_func(draw_bounding_boxes,\n",
    "                            [self._gt_image, self._gt_boxes, self._im_info],\n",
    "                            tf.float32, name=\"gt_boxes\")\n",
    "        return tf.summary.image('GROUND_TRUTH', image)\n",
    "\n",
    "    def _add_act_summary(self,tensor):\n",
    "        tf.summary.histogram('ACT/' + tensor.op.name + '/activations', tensor)\n",
    "        tf.summary.scalar('ACT/' + tensor.op.name + '/zero_fraction',tf.nn.zero_fraction(tensor))\n",
    "    \n",
    "    def _add_score_summary(self,key,tensor):\n",
    "        tf.summary.histogram('SCORE/' + tensor.op.name + '/' + key + '/scores', tensor)\n",
    "    \n",
    "    def _add_train_summary(self,var):\n",
    "        tf.summary.histogram('TRAIN/' + var.op.name, var)\n",
    "    \n",
    "    def _reshape_layer(self,bottom,num_dim, name):\n",
    "        input_shape = tf.shape(bottom)\n",
    "        with tf.variable_scope(name) as scope:\n",
    "            # change the channel to the caffe format\n",
    "            to_caffe = tf.transpose(bottom, [0, 3, 1, 2]) # NCHW where N is batch size, C is channel, H is height, W is width\n",
    "            # then force it to have channel 2\n",
    "            reshaped = tf.reshape(to_caffe, tf.concat(axis=0, values =[[1, num_dim, -1], [input_shape[2]]]))\n",
    "            # then swap the channel back\n",
    "            to_tf = tf.transpose(reshaped, [0, 2, 3, 1])\n",
    "            return to_tf\n",
    "\n",
    "    def _softmax_layer(self,bottom,name):\n",
    "        if name.startswith('rpn_cls_prob_reshape'):\n",
    "            input_shape = tf.shape(bottom)\n",
    "            bottom_reshaped = tf.reshape(bottom, [-1, input_shape[-1]])\n",
    "            reshaped_score = tf.nn.softmax(bottom_reshaped, name=name)\n",
    "            return tf.reshape(reshaped_score, input_shape)\n",
    "        return tf.nn.softmax(bottom, name=name)\n",
    "\n",
    "    def _proposal_layer(self, rpn_cls_prob, rpn_bbox_pred, name):\n",
    "        with tf.name_scope(name) as scope:\n",
    "            rois, rpn_scores = proposal_layer(rpn_cls_prob, rpn_bbox_pred, self.im_info, self._feat_stride, self._anchors, self._num_anchors)\n",
    "            rois.set_shape([None, 5])\n",
    "            rpn_scores.set_shape([None, 1])\n",
    "        return rois, rpn_scores\n",
    "\n",
    "    def _crop_pool_layer(self, bottom, rois, name):\n",
    "        with tf.name_scope(name) as scope:\n",
    "            batch_ids = tf.squeeze(tf.slice(rois, [0,0], [-1,1], name=\"batch_id\"), [1])\n",
    "            # Get the normalized coordinates of bounding boxes\n",
    "            bottom_shape = tf.shape(bottom)\n",
    "            height = (tf.cast(bottom_shape[1], tf.float32) - 1.) * np.float32(self._feat_stride[0])\n",
    "            width = (tf.cast(bottom_shape[2], tf.float32) - 1.) * np.float32(self._feat_stride[0])\n",
    "            x1 = tf.slice(rois, [0,1], [-1,1], name=\"x1\") / width\n",
    "            y1 = tf.slice(rois, [0,2], [-1,1], name=\"y1\") / height\n",
    "            x2 = tf.slice(rois, [0,3], [-1,1], name=\"x2\") / width\n",
    "            y2 = tf.slice(rois, [0,4], [-1,1], name=\"y2\") / height\n",
    "            bboxes = tf.stop_gradient(tf.concat([y1,x1,y2,x2], axis=1))\n",
    "            pre_pool_size = 7 * 2\n",
    "            crops = tf.image.crop_and_resize(bottom, bboxes, tf.cast(batch_ids,tf.int32), [pre_pool_size, pre_pool_size], name=\"crops\")\n",
    "        return slim.max_pool2d(crops, [2, 2], padding='SAME')\n",
    "\n",
    "        def _anchor_target_layer(self, rpn_cls_score, name):\n",
    "            with tf.name_scope(name) as scope:\n",
    "                rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = tf.py_func(_anchor_target_layer,\n",
    "                [rpn_cls_score, self._gt_boxes, self._im_info, self._feat_stride, self._anchors, self._num_anchors],\n",
    "                [tf.float32, tf.float32, tf.float32, tf.float32], name = \"anchor_target\")\n",
    "\n",
    "                rpn_labels.set_shape([1, 1, None, None])\n",
    "                rpn_bbox_targets.set_shape([1, None, None, self._num_anchors * 4])\n",
    "                rpn_bbox_inside_weights.set_shape([1, None, None, self._num_anchors*4])\n",
    "                rpn_bbox_outside_weights.set_shape([1, None, None, self._num_anchors*4])\n",
    "\n",
    "                rpn_labels = tf.cast(rpn_labels,tf.int32, name = 'to_int32')\n",
    "                self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "                self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "                self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "                self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "\n",
    "                self._score_summaries.update(self._anchor_targets)\n",
    "            return rpn_labels\n",
    "\n",
    "        def _proposal_target_layer(self, rois, roi_scores, name):\n",
    "            with tf.name_scope(name) as scope:\n",
    "                rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights = tf.py_func(proposal_target_layer,\n",
    "                [rois, roi_scores, self._gt_boxes, self._num_classes],\n",
    "                [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32],\n",
    "                name = \"proposal_target\")\n",
    "\n",
    "                rois.set_shape([128,5])\n",
    "                roi_scores.set_shape([128])\n",
    "                labels.set_shape([128,1])\n",
    "                bbox_targets.set_shape([128,self._num_classes*4])\n",
    "                bbox_inside_weights.set_shape([128,self._num_classes*4])\n",
    "                bbox_outside_weights.set_shape([128,self._num_classes*4])\n",
    "\n",
    "                self._proposal_targets['rois'] = rois\n",
    "                self._proposal_targets['labels'] = tf.cast(labels,tf.int32)\n",
    "                self._proposal_targets['bbox_targets'] = bbox_targets\n",
    "                self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\n",
    "                self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\n",
    "\n",
    "                self._score_summaries.update(self._proposal_targets)\n",
    "            return rois, roi_scores\n",
    "        \n",
    "        def _anchor_component(self):\n",
    "            with tf.name_scope('ANCHOR_'+self.tag) as scope:\n",
    "                # just to get the shape right\n",
    "                height = tf.cast((tf.math.ceil(self.im_info[0] / np.float32(self._feat_stride[0]))),tf.int32)\n",
    "                width = tf.cast((tf.math.ceil(self.im_info[1] / np.float32(self._feat_stride[0]))),tf.int32)\n",
    "                anchors, anchor_length = generate_anchors_pre_tf(height, width, self._feat_stride, self._anchor_scales, self._anchor_ratios)\n",
    "                anchors.set_shape([None, 4])\n",
    "                anchor_length.set_shape([])\n",
    "                self._anchors = anchors\n",
    "                self._anchor_length = anchor_length\n",
    "        \n",
    "        def _build_network(self,is_training=True):\n",
    "            initializer = tf.random.truncated_normal(0.0, 0.01)\n",
    "            initializer_bbox = tf.random.truncated_normal(0.0, 0.001)\n",
    "\n",
    "            new_conv = self._image_to_head(is_training)\n",
    "            with tf.name_scope(self._scope, self._scope):\n",
    "                self._anchor_component()\n",
    "                rois = self._region_proposal(net_conv,is_training,initializer)\n",
    "                pool5 = self._crop_pool_layer(net_conv, rois, \"pool5\")\n",
    "            \n",
    "            fc7 = self._head_to_tail(pool5, is_training)\n",
    "            with tf.name_scope(self._scope, self._scope):\n",
    "                cls_prob, bbox_pred = self._region_classification(fc7, is_training, initializer, initializer_bbox)\n",
    "            \n",
    "            self._score_summaries.update(self._predictions)\n",
    "\n",
    "            return rois, cls_prob, bbox_pred\n",
    "        \n",
    "        def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=0.1, dim=[1]):\n",
    "            sigma_2 = sigma**2\n",
    "            box_diff = bbox_pred - bbox_targets\n",
    "            in_box_diff = bbox_inside_weights * box_diff\n",
    "            abs_in_box_diff = tf.math.abs(in_box_diff)\n",
    "            smoothL1_sign = tf.stop_gradient(tf.cast(tf.less(abs_in_box_diff,1.0 / sigma_2),tf.float32))\n",
    "            in_loss_box = tf.math.pow(in_box_diff,2) * (sigma_2 / 2.) * smoothL1_sign + (abs_in_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n",
    "            out_loss_box = bbox_outside_weights * in_loss_box\n",
    "            loss_box = tf.reduce_mean(tf.reduce_sum(out_loss_box, axis=dim))\n",
    "            return loss_box\n",
    "\n",
    "        def _region_proposal(self, net_conv, is_training, initializer):\n",
    "            rpn = slim.conv2d(net_conv, 512, [3,3], trainable=is_training, weights_initializer=initializer, scope=\"rpn_conv/3x3\")\n",
    "            self._act_summaries.append(rpn)\n",
    "            rpn_cls_score = slim.conv2d(rpn, self._num_anchors*2, [1,1], trainable=is_training, weights_initializer=initializer,padding='VALID', activation_fn=None, scope='rpn_cls_score')\n",
    "            rpn_cls_score_reshape = self._reshape_layer(rpn_cls_score, 2, 'rpn_cls_score_reshape')\n",
    "            rpn_cls_prob_reshape = self._softmax_layer(rpn_cls_score_reshape, \"rpn_cls_prob_reshape\")\n",
    "            rpn_cls_pred = tf.argmax(tf.reshape(rpn_cls_score_reshape, [-1, 2]), axis=1, name = \"rpn_cls_pred\")\n",
    "            rpn_cls_prob = self._reshape_layer(rpn_cls_prob_reshape, self._num_anchors*2, 'rpn_cls_prob')\n",
    "            rpn_bbox_pred = slim.conv2d(rpn, self._num_anchors*4, [1,1], trainable=is_training, weights_initializer=initializer, padding='VALID', activation_fn=None, scope='rpn_bbox_pred')\n",
    "            \n",
    "            if is_training:\n",
    "                rois, roi_scores = self._proposal_layer(rpn_cls_prob, rpn_bbox_pred, \"rois\")\n",
    "                rpn_labels = self._anchor_target_layer(rpn_cls_score, \"anchor\")\n",
    "                # Try to have a deterministic order for the computing graph, for reproducibility\n",
    "                with tf.control_dependencies([rpn_labels]):\n",
    "                    rois, _ = self._proposal_target_layer(rois, roi_scores, \"rpn_rois\")\n",
    "            \n",
    "            self._predictions[\"rpn_cls_score\"] = rpn_cls_score\n",
    "            self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\n",
    "            self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\n",
    "            self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\n",
    "            self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\n",
    "            self._predictions[\"rois\"] = rois\n",
    "\n",
    "            return rois\n",
    "\n",
    "        def _region_classification(self, fc7, is_training, initializer, initializer_bbox):\n",
    "            cls_score = slim.fully_connected(fc7, self._num_classes, weights_initializer= initializer, trainable= is_training, activation_fn=None, scope='cls_score')\n",
    "            cls_prob = self._softmax_layer(cls_score, \"cls_prob\")\n",
    "            cls_pred = tf.argmax(cls_score, axis=1, name=\"cls_pred\")\n",
    "            bbox_pred = slim.fully_connected(fc7, self._num_classes*4, weights_initializer= initializer_bbox, trainable= is_training, activation_fn=None, scope='bbox_pred')\n",
    "            self._predictions['cls_score'] = cls_score\n",
    "            self._predictions['cls_pred'] = cls_pred\n",
    "            self._predictions['cls_prob'] = cls_prob\n",
    "            self._predictions['bbox_pred'] = bbox_pred\n",
    "\n",
    "            return cls_prob, bbox_pred\n",
    "        \n",
    "        def _image_to_head(self,is_training,resuse=None):\n",
    "            raise NotImplementedError\n",
    "        def _head_to_tail(self,pool5,is_training,reuse=None):\n",
    "            raise NotImplementedError\n",
    "        \n",
    "\n",
    "        def create_architecture(self, mode, num_classes, tag=None, anchor_scales = (8,16,32), _anchor_ratios = (0.5,1,2)):\n",
    "            self._image = tf.compat.v1.placeholder(tf.float32, shape=[1, None, None, 3])\n",
    "            self._im_info = tf.compat.v1.placeholder(tf.float32, shape=[3])\n",
    "            self._gt_boxes = tf.compat.v1.placeholder(tf.float32, shape=[None, 5])\n",
    "            self._tag = tag\n",
    "\n",
    "            self._num_classes = num_classes\n",
    "            self._mode = mode\n",
    "            self._anchor_scales = anchor_scales\n",
    "            self._num_scales = len(anchor_scales)\n",
    "\n",
    "            self._anchor_ratios = _anchor_ratios\n",
    "            self._num_ratios = len(anchor_ratios)\n",
    "\n",
    "            self._num_anchors = self._num_scales * self._num_ratios\n",
    "\n",
    "            training = mode == 'TRAIN'\n",
    "\n",
    "            assert tag!=None\n",
    "\n",
    "            # handle most of the regularizers here\n",
    "            weights_regularizer = tf.keras.regularizers.l2(0.0001)\n",
    "            biases_regularizer = tf.compat.v1.no_regularizer\n",
    "\n",
    "            with arg_scope([slim.conv2d, slim.conv2d_in_plane, slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected], weights_regularizer = weights_regularizer, biases_regularizer= biases_regularizer, biases_initializer = tf.constant_initializer(0.0)):\n",
    "                rois, cls_prob, bbox_pred = self._build_network(training)\n",
    "            layers_to_output = {'rois': rois}\n",
    "\n",
    "            for var in tf.Variable(trainable=True):\n",
    "                self._train_summaries.append(var)\n",
    "            \n",
    "            self._add_losses()\n",
    "            layers_to_output.update(self._losses)\n",
    "\n",
    "            val_summaries = []\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                val_summaries.append(self._add_gt_image_summary())\n",
    "                for key, var in self._event_summaries.items():\n",
    "                    val_summaries.append(tf.summary.scalar(key, var))\n",
    "                for key, var in self._score_summaries.items():\n",
    "                    self._add_score_summary(key, var)\n",
    "                for var in self._act_summaries:\n",
    "                    self._add_act_summary(var)\n",
    "                for var in self._train_summaries:\n",
    "                    self._add_train_summary(var)\n",
    "            \n",
    "            self._summary_op = tf.summary.merge_all()\n",
    "            self._summary_op_val = tf.summary.merge(val_summaries)\n",
    "\n",
    "            layers_to_output.update(self._predictions)\n",
    "            return layers_to_output\n",
    "\n",
    "        def train_step(self,sess,blobs,train_op):\n",
    "            feed_dict={self._image: blobs['data'], self._im_info: blobs['im_info'], self._gt_boxes: blobs['gt_boxes']}\n",
    "            rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, _ = sess.run([self._losses[\"rpn_cross_entropy\"], self._losses['rpn_loss_box'], self._losses['cross_entropy'], self._losses['loss_box'], self._losses['total_loss'], train_op], feed_dict=feed_dict)\n",
    "            return rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss\n",
    "\n",
    "\n",
    "        def train_step_with_summary(self, sess, blobs, train_op):\n",
    "            feed_dict = {self._image: blobs['data'], self._im_info: blobs['im_info'], self._gt_boxes: blobs['gt_boxes']}\n",
    "            rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, summary, _ = sess.run([self._losses['rpn_cross_entropy'], self._losses['rpn_loss_box'], self._losses['cross_entropy'], self._losses['loss_box'], self._losses['total_loss'], self._summary_op, train_op], feed_dict=feed_dict)\n",
    "            return rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "from tf_slim import losses\n",
    "from tf_slim import arg_scope\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg19(Network):\n",
    "    def __init__(self):\n",
    "        Network.__init__(self)\n",
    "        self._feat_stride = [16, ]\n",
    "        self._feat_compress = [1. / float(self._feat_stride[0]), ]\n",
    "        self._scope = 'vgg_19'\n",
    "    \n",
    "    def _image_to_head(self, is_training, reuse=None):\n",
    "        with tf.name_scope(self._scope, self._scope, reuse=reuse):\n",
    "            net = slim.repeat(self._image, 2, slim.conv2d, 64, [3, 3], trainable=False, scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3,3], trainable=False, scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool2')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3,3], trainable=is_training, scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool3')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], trainable=is_training, scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool4')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], trainable=is_training, scope='conv5')\n",
    "        \n",
    "        self._act_summaries.append(net)\n",
    "        self.layers['head'] = net\n",
    "\n",
    "        return net\n",
    "\n",
    "    def _head_to_tail(self,pool5,is_training, reuse=None):\n",
    "        with tf.name_scope(self._scope, self._scope, reuse=reuse):\n",
    "            pool5_flat = slim.flatten(pool5, scope='flatten')\n",
    "            fc6 = slim.fully_connected(pool5_flat, 4096, scope='fc6')\n",
    "            if is_training:\n",
    "                fc6 = slim.dropout(fc6, keep_prob=0.5, is_training=True, scope='dropout6')\n",
    "            fc7 = slim.fully_connected(fc6, 4096, scope='fc7')\n",
    "            if is_training:\n",
    "                fc7 = slim.dropout(fc7, keep_prob=0.5, is_training=True, scope='dropout7')\n",
    "        return fc7;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roidb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_roidb(imdb):\n",
    "    \"\"\"Enrich the imdb's roidb by adding some derived quantities that\n",
    "    are useful for training. This function precomputes the maximum\n",
    "    overlap, taken over ground-truth boxes, between each ROI and\n",
    "    each ground-truth box. The class with maximum overlap is also\n",
    "    recorded.\n",
    "    \"\"\"\n",
    "    roidb = imdb.roidb\n",
    "    if not (imdb.name.startswith('coco')):\n",
    "        sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n",
    "                    for i in range(imdb.num_images)]\n",
    "    for i in range(len(imdb.image_index)):\n",
    "        roidb[i]['image'] = imdb.image_path_at(i)\n",
    "        if not (imdb.name.startswith('coco')):\n",
    "            roidb[i]['width'] = sizes[i][0]\n",
    "            roidb[i]['height'] = sizes[i][1]\n",
    "        # need gt_overlaps as a dense array for argmax\n",
    "        gt_overlaps = roidb[i]['gt_overlaps'].toarray()\n",
    "        # max overlap with gt over classes (columns)\n",
    "        max_overlaps = gt_overlaps.max(axis=1)\n",
    "        # gt class that had the max overlap\n",
    "        max_classes = gt_overlaps.argmax(axis=1)\n",
    "        roidb[i]['max_classes'] = max_classes\n",
    "        roidb[i]['max_overlaps'] = max_overlaps\n",
    "        # sanity checks\n",
    "        # max overlap of 0 => class should be zero (background)\n",
    "        zero_inds = np.where(max_overlaps == 0)[0]\n",
    "        assert all(max_classes[zero_inds] == 0)\n",
    "        # max overlap > 0 => class should not be zero (must be a fg class)\n",
    "        nonzero_inds = np.where(max_overlaps > 0)[0]\n",
    "        assert all(max_classes[nonzero_inds] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _im_list_to_blob(ims):\n",
    "    \"\"\" Convert a list of images into a network input.\n",
    "    Assumes images are already prepare (mean substracted, BGR order, ....).\n",
    "    \"\"\"\n",
    "\n",
    "    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n",
    "    num_images = len(ims)\n",
    "    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),dtype=np.float32)\n",
    "    for i in range(num_images):\n",
    "        im = ims[i]\n",
    "        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n",
    "    return blob\n",
    "\n",
    "    def prep_im_for_blob(im, pixel_means, target_size, max_size):\n",
    "        \"\"\" mean subtact and scale an image for use in a blob\"\"\"\n",
    "        im = im.astype(np.float32, copy=False)\n",
    "        im -= pixel_means\n",
    "        im_shape = im.shape\n",
    "        im_size_min = np.min(im_shape[0:2])\n",
    "        im_size_max = np.max(im_shape[0:2])\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > max_size:\n",
    "            im_scale = float(max_size) / float(im_size_max)\n",
    "        im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n",
    "                        interpolation=cv2.INTER_LINEAR)\n",
    "        return im, im_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch blobs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALES = (600,)\n",
    "pixel_means = np.array([[[102.9801, 115.9465, 122.7717]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(roidb,num_classes):\n",
    "    \"\"\"Given a roidb, construct a minibatch sampled from it\"\"\"\n",
    "    num_images = len(roidb)\n",
    "    # Sample random scales to use for each image in this batch\n",
    "    random_scale_inds = npr.randint(0,high=len(SCALES),size=num_images)\n",
    "    assert(128%num_images == 0), 'num_images ({}) must divide 128'.format(num_images)\n",
    "\n",
    "    # Get the input image blob, formatted for caffe\n",
    "    im_blob, im_scale = _get_image_blob(roidb, random_scale_inds)\n",
    "    blobs = {'data':im_blob}\n",
    "    assert(len(im_scale) == 1), 'Single batch only'\n",
    "    assert(len(roidb) == 1), 'Single batch only'\n",
    "\n",
    "    if USE_ALL_GT:\n",
    "        gt_inds = np.where(roidb[0]['gt_classes'] != 0)[0]\n",
    "    else:\n",
    "        gt_inds = np.where(roidb[0]['gt_classes'] != 0 & np.all(roidb[0]['gt_overlaps'].toarray() > -1.0, axis=1))[0]\n",
    "    gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n",
    "    gt_boxes[:, 0:4] = roidb[0]['boxes'][gt_inds, :] * im_scale[0]\n",
    "    gt_boxes[:, 4] = roidb[0]['gt_classes'][gt_inds]\n",
    "    blobs['gt_boxes'] = gt_boxes\n",
    "    blobs['im_info'] = np.array(\n",
    "        [im_blob.shape[1], im_blob.shape[2], im_scale[0]],\n",
    "        dtype=np.float32)\n",
    "    return blobs\n",
    "\n",
    "def _get_image_blob(roidb, scale_inds):\n",
    "    \"\"\"Builds an input blob from the images in the roidb at the specified\n",
    "    scales.\n",
    "    \"\"\"\n",
    "    num_images = len(roidb)\n",
    "    processed_ims = []\n",
    "    im_scales = []\n",
    "    for i in range(num_images):\n",
    "        im = cv2.imread(roidb[i]['image'])\n",
    "        if roidb[i]['flipped']:\n",
    "            im = im[:, ::-1, :]\n",
    "        target_size = SCALES[scale_inds[i]]\n",
    "        im, im_scale = prep_im_for_blob(im, pixel_means, target_size, 1000)\n",
    "        im_scales.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = _im_list_to_blob(processed_ims)\n",
    "    return blob, im_scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoIDataLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIDataLayer(object):\n",
    "    \"\"\"\n",
    "    It implements a caffe Python Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, roidb, num_classes, random=False):\n",
    "        \"\"\"\n",
    "        set the roidb to be used by this layer during training\n",
    "        \"\"\"\n",
    "        self._roidb = roidb\n",
    "        self._num_classes = num_classes\n",
    "        self._random = random\n",
    "        self._shuffle_roidb_inds()\n",
    "    \n",
    "    def _shuffle_roidb_inds(self):\n",
    "        \"\"\"Randomly permute the training roidb\"\"\"\n",
    "        if self._random:\n",
    "            st0 = np.random.get_state()\n",
    "            millis = int(round(time.time()*1000)) % 4294967295\n",
    "            np.random.seed(millis)\n",
    "        self._perm = np.random.permutation(np.arange(len(self._roidb)))\n",
    "        if self._random:\n",
    "            np.random.set_state(st0)\n",
    "        \n",
    "        self._cur = 0\n",
    "    \n",
    "    def _get_next_minibatch_inds(self):\n",
    "        \"\"\"Return the roidb indices for the next minibatch.\"\"\"\n",
    "        if self._cur + 1 >= len(self._roidb):\n",
    "            self._shuffle_roidb_inds()\n",
    "        \n",
    "        db_inds = self._perm[self._cur:self._cur + 1]\n",
    "        self._cur += 1\n",
    "        return db_inds\n",
    "    \n",
    "    def _get_next_minibatch(self):\n",
    "        \"\"\"Return the blobs to be used for the next minibatch.\n",
    "        \"\"\"\n",
    "        db_inds = self._get_next_minibatch_inds()\n",
    "        minibatch_db = [self._roidb[i] for i in db_inds]\n",
    "        return get_minibatch(minibatch_db, self._num_classes)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"Get blobs and copy them into this layer's top blob vector.\"\"\"\n",
    "        blobs = self._get_next_minibatch()\n",
    "        return blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Timer(object):\n",
    "    def __init__(self):\n",
    "        self.total_time = 0.\n",
    "        self.calls = 0\n",
    "        self.start_time = 0.\n",
    "        self.diff = 0.\n",
    "        self.average_time = 0.\n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def toc(self, average=True):\n",
    "        self.diff = time.time() - self.start_time\n",
    "        self.total_time += self.diff\n",
    "        self.calls += 1\n",
    "        self.average_time = self.total_time / self.calls\n",
    "        if average:\n",
    "            return self.average_time\n",
    "        else:\n",
    "            return self.diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainval - SolveWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import pywrap_tensorflow\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolveWrapper(object):\n",
    "    \"\"\"\n",
    "    A wrapper class for the training purpose\n",
    "    \"\"\"\n",
    "    def __init__(self,sess,network,imdb,roidb,valroidb,output_dir,tbdir,pretrained_model=None):\n",
    "        self.net = network\n",
    "        self.imdb = imdb\n",
    "        self.roidb = roidb\n",
    "        self.valroidb = valroidb\n",
    "        self.output_dir = output_dir\n",
    "        self.tbdir = tbdir\n",
    "        self.tbvaldir = tbdir + '_val'\n",
    "        if not os.path.exists(self.tbvaldir):\n",
    "            os.makedirs(self.tbvaldir)\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "    def filter_roidb(roidb):\n",
    "        \"\"\"Remove roidb entries that have no usable RoIs\"\"\"\n",
    "        def is_valid(entry):\n",
    "            overlaps = entry['max_overlaps']\n",
    "            fg_inds = np.where(overlaps>=0.5)[0]\n",
    "            bg_inds = np.where((overlaps<0.5) & (overlaps>0.1))[0]\n",
    "            valid = len(fg_inds)>0 or len(bg_inds)>0\n",
    "            return valid\n",
    "        \n",
    "        num = len(roidb)\n",
    "        filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n",
    "        num_after = len(filtered_roidb)\n",
    "        print('Filtered {} roidb entries: {} -> {}'.format(num-num_after,num,num_after))\n",
    "        return filtered_roidb\n",
    "\n",
    "    def train_model(self, sess, max_iters):\n",
    "        # Build data layers for both training and validation set\n",
    "        self.data_layer = RoIDataLayer(self.roidb, self.imdb.num_classes)\n",
    "        self.data_layer_val = RoIDataLayer(self.valroidb, self.imdb.num_classes, random=True)\n",
    "\n",
    "        # Construct the computation graph\n",
    "        lr, train_op = self.construct_graph()\n",
    "\n",
    "        # Find previous snapshots if there is any to restore from\n",
    "        lsf, nfiles, sfiles = self.find_previous()\n",
    "\n",
    "        # Initialize the variables or restore them from the last snapshot\n",
    "        if lsf == 0:\n",
    "            rate, last_snapshot_iter, stepsizes, np_path, ss_paths = self.initialize(sess);\n",
    "        else:\n",
    "            rate, last_snapshot_iter, stepsizes, np_path, ss_paths = self.restore(sess, str(lsf-1))\n",
    "        timer = Timer();\n",
    "        iter = last_snapshot_iter + 1\n",
    "        last_summary_time = time.time()\n",
    "\n",
    "        stepsizes.append(max_iters)\n",
    "        stepsizes.reverse()\n",
    "        next_stepsize = stepsizes.pop()\n",
    "        while iters < max_iters+1:\n",
    "            if iter == next_stepsize_1:\n",
    "                self.snapshot(sess, iter)\n",
    "                rate *= 0.1\n",
    "                sess.run(tf.assign(lr, rate))\n",
    "                next_stepsize = stepsizes.pop()\n",
    "            \n",
    "            timer.tic()\n",
    "            blobs = self.data_layer.forward()\n",
    "\n",
    "            now = time.time()\n",
    "            if iter == 1 or now-last_summary_time > 180:\n",
    "                rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, total_loss, summary = self.net.train_step_with_summary(sess, blobs, train_op)\n",
    "                self.writer.add_summary(summary, float(iter))\n",
    "                # Also check the summary on the validation set\n",
    "                blobs_val = self.data_layer_val.forward()\n",
    "                summary_val = self.net.get_summary(sess, blobs_val)\n",
    "                self.valwriter.add_summary(summary_val, float(iter))\n",
    "                last_summary_time = now\n",
    "            else:\n",
    "                rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, total_loss = self.net.train_step(sess, blobs, train_op)\n",
    "            timer.toc()\n",
    "\n",
    "            # Display training information\n",
    "            if iter % 10 == 0:\n",
    "                print('iter: %d / %d, total loss: %.6f\\n >>> rpn_loss_cls: %.6f\\n '\n",
    "                      '>>> rpn_loss_box: %.6f\\n >>> loss_cls: %.6f\\n >>> loss_box: %.6f\\n >>> lr: %f' % \\\n",
    "                      (iter, max_iters, total_loss, rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, lr.eval()))\n",
    "                print('speed: {:.3f}s / iter'.format(timer.average_time))\n",
    "            \n",
    "            # Snapshotting\n",
    "            if iter % 5000 == 0:\n",
    "                last_snapshot_iter = iter\n",
    "                ss_path , np_path = self.snapshot(sess, iter)\n",
    "                np_paths.append(np_path)\n",
    "                ss_paths.append(ss_path)\n",
    "\n",
    "                if len(np_paths) > 5:\n",
    "                    self.remove_snapshot(np_paths, ss_paths)\n",
    "            \n",
    "            iter += 1\n",
    "\n",
    "            if last_snapshot_iter != iter-1:\n",
    "                self.snapshot(sess,iter-1)\n",
    "            \n",
    "            self.writer.close()\n",
    "            self.valwriter.close()\n",
    "\n",
    "\n",
    "    def train_net(network, imdb, roidb, valroidb, output_dir, tb_dir, pretrained_model=None, max_iters=40000):\n",
    "        roidb = filter_roidb(roidb)\n",
    "        valroidb = filter_roidb(valroidb)\n",
    "\n",
    "        tfconfig = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "        tfconfig.gpu_options.allow_growth = True\n",
    "\n",
    "        with tf.compat.v1.Session(config=tfconfig) as sess:\n",
    "            sw = SolveWrapper(sess,network,imdb,roidb,valroidb,output_dir,tb_dir,pretrained_model)\n",
    "            print('Solving')\n",
    "            sw.train_model(sess,max_iters)\n",
    "            print('done solving')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578ce72e72bd9f13049fd4c13d9f5b1c81715c13ddea0a3c61ff70756cb5d6d4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
